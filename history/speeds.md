# Model Benchmarks
To calculate how fast the response is generated in tokens per second (token/s), divide eval_count / eval_duration * 10^9

## llama3.2
46.303776417782226443200044136366